# Example environment configuration
# Copy this file to .env and modify as needed

# Server Configuration
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# Model Configuration
MODEL_PATH=./models/phi-3-mini-4k-instruct-q4.gguf

# llama-cpp-python Configuration
# Set to -1 to offload all layers to GPU (recommended for M2)
N_GPU_LAYERS=32
N_CTX=4096
N_BATCH=512

# Dynamic Batching Configuration
MAX_BATCH_SIZE=8
BATCH_TIMEOUT=0.1

# Generation Defaults
DEFAULT_MAX_TOKENS=256
DEFAULT_TEMPERATURE=0.7
DEFAULT_TOP_P=0.9
DEFAULT_TOP_K=40

# Performance Configuration
VERBOSE=false
